{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b4f0f-48b0-471e-9b46-ab4fa79e1045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Knowledge Base Module - Centralized data management for Clever AI\n",
    "\n",
    "Why: Provides advanced database operations beyond basic DatabaseManager including\n",
    "chat history, knowledge sources, content chunks, user preferences, and personality\n",
    "state management. Enables Clever to build comprehensive memory and learning\n",
    "capabilities while maintaining thread-safe operations and single database architecture.\n",
    "Where: Used by app.py for chat interactions, evolution_engine for learning analysis,\n",
    "sync modules for knowledge ingestion, and persona.py for context retrieval. Acts as\n",
    "high-level database API layer above DatabaseManager for specialized Clever operations.\n",
    "How: Extends DatabaseManager functionality with specialized tables and operations\n",
    "for Clever's specific data needs including interaction logging, content search,\n",
    "preference management, and personality tracking using centralized DB_PATH configuration.\n",
    "\n",
    "Connects to:\n",
    "    - database.py: Uses centralized DatabaseManager for all database operations\n",
    "    - config.py: Uses DB_PATH for single database configuration  \n",
    "    - app.py: Chat history logging, context retrieval, and user preference management\n",
    "    - evolution_engine.py: Knowledge storage, interaction analysis, and personality state tracking\n",
    "    - persona.py: Context building and knowledge-aware response generation\n",
    "    - sync modules: Knowledge source registration and content chunk management\n",
    "    - nlp_processor.py: Stores processed keywords, entities, and analysis results\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "from database import DatabaseManager\n",
    "import config\n",
    "\n",
    "# Thread-safe database operations - reuse from DatabaseManager\n",
    "_db_lock = threading.RLock()\n",
    "\n",
    "# Global database instance\n",
    "_db_manager = None\n",
    "\n",
    "def init_db() -> bool:\n",
    "    \"\"\"\n",
    "    Initialize the knowledge base database with extended tables\n",
    "    \n",
    "    Why: Creates specialized tables for chat history, knowledge sources, user\n",
    "    preferences, and personality state that extend beyond basic DatabaseManager\n",
    "    functionality for comprehensive Clever AI memory and learning.\n",
    "    Where: Called at module import to ensure database schema is ready for\n",
    "    knowledge base operations used by app.py and evolution_engine.py.\n",
    "    How: Uses centralized DatabaseManager to create tables with proper schema\n",
    "    for advanced Clever AI functionality while maintaining single DB architecture.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if initialization successful, False otherwise\n",
    "        \n",
    "    Connects to:\n",
    "        - database.py: Uses DatabaseManager for thread-safe database operations\n",
    "        - config.py: Uses centralized DB_PATH configuration\n",
    "    \"\"\"\n",
    "    global _db_manager\n",
    "    \n",
    "    try:\n",
    "        with _db_lock:\n",
    "            # Use centralized DatabaseManager - no direct sqlite3 connections\n",
    "            _db_manager = DatabaseManager(config.DB_PATH)\n",
    "            \n",
    "            # Use DatabaseManager's connection for table creation\n",
    "            with _db_manager._connect() as conn:\n",
    "                cursor = conn.cursor()\n",
    "            \n",
    "            # Create interactions table for chat history\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS interactions (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    timestamp TEXT NOT NULL,\n",
    "                    user_message TEXT NOT NULL,\n",
    "                    clever_response TEXT NOT NULL,\n",
    "                    intent_detected TEXT,\n",
    "                    sentiment_compound REAL,\n",
    "                    nlp_analysis TEXT\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # Create knowledge sources table\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS knowledge_sources (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    filename TEXT NOT NULL,\n",
    "                    file_path TEXT,\n",
    "                    content_type TEXT,\n",
    "                    processed_date TEXT,\n",
    "                    file_size INTEGER,\n",
    "                    content_hash TEXT\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # Create content chunks table\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS content_chunks (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    source_id INTEGER,\n",
    "                    chunk_index INTEGER,\n",
    "                    content TEXT NOT NULL,\n",
    "                    embedding_vector TEXT,\n",
    "                    keywords TEXT,\n",
    "                    entities TEXT,\n",
    "                    FOREIGN KEY (source_id) REFERENCES knowledge_sources (id)\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # Create user preferences table\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS user_preferences (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    preference_key TEXT UNIQUE NOT NULL,\n",
    "                    preference_value TEXT,\n",
    "                    last_updated TEXT\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # Create personality state table\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS personality_state (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    emotional_state TEXT,\n",
    "                    mood_score REAL,\n",
    "                    interaction_count INTEGER,\n",
    "                    last_updated TEXT\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # Create system metrics table\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS system_metrics (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    metric_name TEXT NOT NULL,\n",
    "                    metric_value REAL,\n",
    "                    metric_data TEXT,\n",
    "                    timestamp TEXT\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            conn.commit()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Database initialization failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def log_interaction(user_message: str, clever_response: str, intent_detected: str = None,\n",
    "                   sentiment_compound: float = None, nlp_analysis: Dict = None) -> int:\n",
    "    \"\"\"\n",
    "    Log a chat interaction to the knowledge base\n",
    "    \n",
    "    Why: Stores chat interactions for learning, memory, and conversation history\n",
    "    that enables Clever to build context and improve responses over time.\n",
    "    Where: Called by app.py after each user interaction to maintain complete\n",
    "    conversation history and enable evolution_engine learning.\n",
    "    How: Uses centralized DatabaseManager to insert interaction data into the\n",
    "    interactions table with structured metadata for NLP analysis.\n",
    "    \n",
    "    Args:\n",
    "        user_message: The user's input message\n",
    "        clever_response: Clever's generated response\n",
    "        intent_detected: Optional detected intent category\n",
    "        sentiment_compound: Optional sentiment score [-1.0, 1.0]\n",
    "        nlp_analysis: Optional dictionary of NLP analysis results\n",
    "        \n",
    "    Returns:\n",
    "        int: Interaction ID if successful, -1 if failed\n",
    "        \n",
    "    Connects to:\n",
    "        - database.py: Uses DatabaseManager for thread-safe database operations\n",
    "        - app.py: Receives interaction data from main application flow\n",
    "        - evolution_engine.py: Provides data for learning analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with _db_lock:\n",
    "            with _db_manager._connect() as conn:\n",
    "                cursor = conn.cursor()\n",
    "            \n",
    "                # Use the existing schema with user_input and action_taken columns\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO interactions \n",
    "                    (timestamp, user_input, action_taken, active_mode, parsed_data)\n",
    "                    VALUES (?, ?, ?, ?, ?)\n",
    "                ''', (\n",
    "                    datetime.now().isoformat(),\n",
    "                    user_message,\n",
    "                    clever_response,  # Store response in action_taken\n",
    "                    intent_detected or 'chat',  # Store intent in active_mode\n",
    "                    json.dumps({\n",
    "                        'sentiment_compound': sentiment_compound,\n",
    "                        'nlp_analysis': nlp_analysis\n",
    "                    }) if (sentiment_compound is not None or nlp_analysis) else None\n",
    "                ))\n",
    "            \n",
    "            interaction_id = cursor.lastrowid\n",
    "            conn.commit()\n",
    "            \n",
    "            return interaction_id\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to log interaction: {e}\")\n",
    "        return -1\n",
    "\n",
    "def get_recent_interactions(limit: int = 10) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve recent chat interactions from the knowledge base\n",
    "    \n",
    "    Why: Provides conversation history for context building and memory recall\n",
    "    that enables Clever to maintain conversation continuity and reference\n",
    "    previous interactions for more coherent responses.\n",
    "    Where: Used by app.py for context building and evolution_engine for\n",
    "    analyzing interaction patterns and learning opportunities.\n",
    "    How: Queries the interactions table using DatabaseManager with proper\n",
    "    field mapping to return structured conversation history data.\n",
    "    \n",
    "    Args:\n",
    "        limit: Maximum number of recent interactions to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of interaction dictionaries with conversation data\n",
    "        \n",
    "    Connects to:\n",
    "        - database.py: Uses DatabaseManager for thread-safe database queries\n",
    "        - app.py: Provides context for response generation\n",
    "        - evolution_engine.py: Supplies data for learning analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with _db_lock:\n",
    "            with _db_manager._connect() as conn:\n",
    "                cursor = conn.cursor()\n",
    "                \n",
    "                cursor.execute('''\n",
    "                    SELECT timestamp, user_input, action_taken, active_mode, parsed_data\n",
    "                    FROM interactions \n",
    "                    ORDER BY timestamp DESC \n",
    "                    LIMIT ?\n",
    "                ''', (limit,))\n",
    "                \n",
    "                interactions = []\n",
    "                for row in cursor.fetchall():\n",
    "                    timestamp, user_input, action_taken, active_mode, parsed_data_json = row\n",
    "                    \n",
    "                    parsed_data = {}\n",
    "                    if parsed_data_json:\n",
    "                        try:\n",
    "                            parsed_data = json.loads(parsed_data_json)\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    interactions.append({\n",
    "                        'timestamp': timestamp,\n",
    "                        'user_message': user_input,  # Map to expected field name\n",
    "                        'clever_response': action_taken,  # Map to expected field name\n",
    "                        'intent_detected': active_mode,\n",
    "                        'sentiment_compound': parsed_data.get('sentiment_compound'),\n",
    "                        'nlp_analysis': parsed_data.get('nlp_analysis')\n",
    "                    })\n",
    "                \n",
    "                return interactions\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get recent interactions: {e}\")\n",
    "        return []\n",
    "\n",
    "def add_knowledge_source(filename: str, file_path: str = None, content_type: str = None,\n",
    "                        file_size: int = None, content_hash: str = None) -> int:\n",
    "    \"\"\"\n",
    "    Add a knowledge source to the database\n",
    "    \n",
    "    Why: Tracks ingested files and documents to build Clever's knowledge base\n",
    "    and prevent duplicate processing while maintaining source attribution\n",
    "    for generated responses that reference learned material.\n",
    "    Where: Used by pdf_ingestor.py and file_ingestor.py during content\n",
    "    ingestion to register new sources before content chunking.\n",
    "    How: Inserts source metadata into knowledge_sources table using\n",
    "    DatabaseManager with timestamp and hash tracking for deduplication.\n",
    "    \n",
    "    Args:\n",
    "        filename: Name of the source file\n",
    "        file_path: Optional full path to source file\n",
    "        content_type: Optional MIME type or file type\n",
    "        file_size: Optional file size in bytes\n",
    "        content_hash: Optional hash for duplicate detection\n",
    "        \n",
    "    Returns:\n",
    "        int: Source ID if successful, -1 if failed\n",
    "        \n",
    "    Connects to:\n",
    "        - database.py: Uses DatabaseManager for thread-safe operations\n",
    "        - pdf_ingestor.py: Registers PDF sources during ingestion\n",
    "        - file_ingestor.py: Registers text file sources during ingestion\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with _db_lock:\n",
    "            with _db_manager._connect() as conn:\n",
    "                cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute('''\n",
    "                INSERT INTO knowledge_sources \n",
    "                (filename, file_path, content_type, processed_date, file_size, content_hash)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                filename,\n",
    "                file_path,\n",
    "                content_type,\n",
    "                datetime.now().isoformat(),\n",
    "                file_size,\n",
    "                content_hash\n",
    "            ))\n",
    "            \n",
    "            source_id = cursor.lastrowid\n",
    "            conn.commit()\n",
    "            \n",
    "            return source_id\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to add knowledge source: {e}\")\n",
    "        return -1\n",
    "\n",
    "def add_content_chunk(source_id: int, chunk_index: int, content: str,\n",
    "                     keywords: List[str] = None, entities: List[str] = None) -> int:\n",
    "    \"\"\"\n",
    "    Add a content chunk to the database for a knowledge source\n",
    "    \n",
    "    Why: Breaks down large documents into searchable chunks with extracted\n",
    "    keywords and entities to enable efficient retrieval and context-aware\n",
    "    responses based on ingested knowledge.\n",
    "    Where: Called by ingestor modules after processing documents to store\n",
    "    processed content chunks with NLP-extracted metadata for retrieval.\n",
    "    How: Stores content chunks with JSON-serialized keywords and entities\n",
    "    using DatabaseManager, linked to parent source via foreign key.\n",
    "    \n",
    "    Args:\n",
    "        source_id: ID of the parent knowledge source\n",
    "        chunk_index: Index of this chunk within the source\n",
    "        content: The text content of the chunk\n",
    "        keywords: Optional list of extracted keywords\n",
    "        entities: Optional list of extracted entities\n",
    "        \n",
    "    Returns:\n",
    "        int: Chunk ID if successful, -1 if failed\n",
    "        \n",
    "    Connects to:\n",
    "        - database.py: Uses DatabaseManager for thread-safe operations\n",
    "        - nlp_processor.py: Receives processed keywords and entities\n",
    "        - ingestor modules: Stores processed content chunks\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with _db_lock:\n",
    "            with _db_manager._connect() as conn:\n",
    "                cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute('''\n",
    "                INSERT INTO content_chunks \n",
    "                (source_id, chunk_index, content, keywords, entities)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                source_id,\n",
    "                chunk_index,\n",
    "                content,\n",
    "                json.dumps(keywords) if keywords else None,\n",
    "                json.dumps(entities) if entities else None\n",
    "            ))\n",
    "            \n",
    "            chunk_id = cursor.lastrowid\n",
    "            conn.commit()\n",
    "            \n",
    "            return chunk_id\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to add content chunk: {e}\")\n",
    "        return -1\n",
    "\n",
    "def search_content(query: str, limit: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search content chunks for relevant information\n",
    "    \n",
    "    Why: Enables Clever to find and reference relevant knowledge from ingested\n",
    "    documents to provide informed responses based on learned material rather\n",
    "    than relying solely on pre-trained knowledge.\n",
    "    Where: Used by app.py during response generation when user queries might\n",
    "    benefit from referencing ingested documents and knowledge sources.\n",
    "    How: Performs text search across content chunks, keywords, and entities\n",
    "    using DatabaseManager with JOIN to include source attribution.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text to match against content and metadata\n",
    "        limit: Maximum number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Search results with content, metadata, and source info\n",
    "        \n",
    "    Connects to:\n",
    "        - database.py: Uses DatabaseManager for thread-safe search queries\n",
    "        - app.py: Provides knowledge retrieval for response generation\n",
    "        - persona.py: Supplies context for knowledge-aware responses\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with _db_lock:\n",
    "            with _db_manager._connect() as conn:\n",
    "                cursor = conn.cursor()\n",
    "            \n",
    "            # Simple text search - in production, would use embeddings\n",
    "            cursor.execute('''\n",
    "                SELECT cc.content, cc.keywords, cc.entities, ks.filename\n",
    "                FROM content_chunks cc\n",
    "                JOIN knowledge_sources ks ON cc.source_id = ks.id\n",
    "                WHERE cc.content LIKE ? OR cc.keywords LIKE ? OR cc.entities LIKE ?\n",
    "                LIMIT ?\n",
    "            ''', (f'%{query}%', f'%{query}%', f'%{query}%', limit))\n",
    "            \n",
    "            results = []\n",
    "            for row in cursor.fetchall():\n",
    "                content, keywords_json, entities_json, filename = row\n",
    "                \n",
    "                keywords = []\n",
    "                entities = []\n",
    "                try:\n",
    "                    if keywords_json:\n",
    "                        keywords = json.loads(keywords_json)\n",
    "                    if entities_json:\n",
    "                        entities = json.loads(entities_json)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                results.append({\n",
    "                    'content': content,\n",
    "                    'keywords': keywords,\n",
    "                    'entities': entities,\n",
    "                    'source_filename': filename\n",
    "                })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to search content: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_user_preference(key: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Retrieve a user preference value\n",
    "    \n",
    "    Why: Enables personalization by storing and retrieving Jay's preferences\n",
    "    for response style, behavior settings, and custom configurations that\n",
    "    make Clever more tailored to the single user's needs.\n",
    "    Where: Used by app.py and persona.py to customize behavior based on\n",
    "    stored preferences, ensuring consistent personalized experience.\n",
    "    How: Queries user_preferences table using DatabaseManager to retrieve\n",
    "    preference value by key for runtime behavior customization.\n",
    "    \n",
    "    Args:\n",
    "        key: Preference key identifier\n",
    "        \n",
    "    Returns:\n",
    "        Optional[str]: Preference value if found, None otherwise\n",
    "        \n",
    "    Connects to:\n",
    "        - database.py: Uses DatabaseManager for thread-safe preference queries\n",
    "        - app.py: Retrieves preferences for behavior customization\n",
    "        - persona.py: Adapts response generation based on user preferences\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with _db_lock:\n",
    "            with _db_manager._connect() as conn:\n",
    "                cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute('SELECT preference_value FROM user_preferences WHERE preference_key = ?', (key,))\n",
    "            result = cursor.fetchone()\n",
    "            \n",
    "            return result[0] if result else None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get user preference: {e}\")\n",
    "        return None\n",
    "\n",
    "def set_user_preference(key: str, value: str) -> bool:\n",
    "    \"\"\"\n",
    "    Set a user preference value\n",
    "    \n",
    "    Why: Stores Jay's preferences and settings to enable personalization\n",
    "    and consistent behavior across sessions, allowing Clever to adapt\n",
    "    to user preferences and maintain customized experience.\n",
    "    Where: Used by app.py when user configures settings or when persona.py\n",
    "    learns preferred interaction styles during conversations.\n",
    "    How: Uses INSERT OR REPLACE with DatabaseManager to update user_preferences\n",
    "    table with timestamp tracking for preference management.\n",
    "    \n",
    "    Args:\n",
    "        key: Preference key identifier\n",
    "        value: Preference value to store\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "        \n",
    "    Connects to:\n",
    "        - database.py: Uses DatabaseManager for thread-safe preference updates\n",
    "        - app.py: Stores user configuration changes\n",
    "        - persona.py: Learns and stores interaction preferences\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with _db_lock:\n",
    "            with _db_manager._connect() as conn:\n",
    "                cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute('''\n",
    "                INSERT OR REPLACE INTO user_preferences (preference_key, preference_value, last_updated)\n",
    "                VALUES (?, ?, ?)\n",
    "            ''', (key, value, datetime.now().isoformat()))\n",
    "            \n",
    "            conn.commit()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to set user preference: {e}\")\n",
    "        return False\n",
    "\n",
    "def update_personality_state(emotional_state: str, mood_score: float, interaction_count: int) -> bool:\n",
    "    \"\"\"\n",
    "    Update Clever's personality state\n",
    "    \n",
    "    Why: Tracks Clever's evolving personality and emotional state to enable\n",
    "    consistent personality development and mood-aware responses that create\n",
    "    a more engaging and empathetic user experience.\n",
    "    Where: Used by evolution_engine.py during learning analysis and by\n",
    "    persona.py to adjust response generation based on current personality state.\n",
    "    How: Updates personality_state table using DatabaseManager with fixed ID\n",
    "    for singleton personality tracking with timestamp for state evolution.\n",
    "    \n",
    "    Args:\n",
    "        emotional_state: Current emotional state description\n",
    "        mood_score: Numerical mood score for quantitative tracking\n",
    "        interaction_count: Current total interaction count\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "        \n",
    "    Connects to:\n",
    "        - database.py: Uses DatabaseManager for thread-safe state updates\n",
    "        - evolution_engine.py: Updates personality based on learning analysis\n",
    "        - persona.py: Retrieves state for mood-aware response generation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with _db_lock:\n",
    "            with _db_manager._connect() as conn:\n",
    "                cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute('''\n",
    "                INSERT OR REPLACE INTO personality_state \n",
    "                (id, emotional_state, mood_score, interaction_count, last_updated)\n",
    "                VALUES (1, ?, ?, ?, ?)\n",
    "            ''', (emotional_state, mood_score, interaction_count, datetime.now().isoformat()))\n",
    "            \n",
    "            conn.commit()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to update personality state: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_personality_state() -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve Clever's current personality state\n",
    "    \n",
    "    Why: Provides current personality and emotional state data for mood-aware\n",
    "    response generation and personality consistency across interactions,\n",
    "    enabling Clever to maintain coherent character development.\n",
    "    Where: Used by persona.py for response generation and evolution_engine.py\n",
    "    for personality analysis and development tracking.\n",
    "    How: Queries personality_state table using DatabaseManager for singleton\n",
    "    personality record with structured state data return.\n",
    "    \n",
    "    Returns:\n",
    "        Optional[Dict]: Personality state data if available, None otherwise\n",
    "        \n",
    "    Connects to:\n",
    "        - database.py: Uses DatabaseManager for thread-safe state queries\n",
    "        - persona.py: Retrieves state for personality-aware responses\n",
    "        - evolution_engine.py: Analyzes personality development patterns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with _db_lock:\n",
    "            with _db_manager._connect() as conn:\n",
    "                cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute('SELECT emotional_state, mood_score, interaction_count, last_updated FROM personality_state WHERE id = 1')\n",
    "            result = cursor.fetchone()\n",
    "            \n",
    "            if result:\n",
    "                return {\n",
    "                    'emotional_state': result[0],\n",
    "                    'mood_score': result[1],\n",
    "                    'interaction_count': result[2],\n",
    "                    'last_updated': result[3]\n",
    "                }\n",
    "            \n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get personality state: {e}\")\n",
    "        return None\n",
    "\n",
    "def log_system_metric(metric_name: str, metric_value: float, metric_data: Dict = None) -> bool:\n",
    "    \"\"\"\n",
    "    Log a system performance or behavior metric\n",
    "    \n",
    "    Why: Tracks system performance, behavior patterns, and operational metrics\n",
    "    to enable monitoring, debugging, and performance optimization for\n",
    "    continuous improvement of Clever's operation.\n",
    "    Where: Used by debug_config.py for performance tracking and health_monitor.py\n",
    "    for system status monitoring with metric aggregation.\n",
    "    How: Inserts metric data into system_metrics table using DatabaseManager\n",
    "    with JSON serialization for complex metric data structures.\n",
    "    \n",
    "    Args:\n",
    "        metric_name: Name/identifier of the metric being logged\n",
    "        metric_value: Numerical value of the metric\n",
    "        metric_data: Optional additional metric metadata\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "        \n",
    "    Connects to:\n",
    "        - database.py: Uses DatabaseManager for thread-safe metric logging\n",
    "        - debug_config.py: Logs performance and debugging metrics\n",
    "        - health_monitor.py: Records system health and status metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with _db_lock:\n",
    "            with _db_manager._connect() as conn:\n",
    "                cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute('''\n",
    "                INSERT INTO system_metrics (metric_name, metric_value, metric_data, timestamp)\n",
    "                VALUES (?, ?, ?, ?)\n",
    "            ''', (\n",
    "                metric_name,\n",
    "                metric_value,\n",
    "                json.dumps(metric_data) if metric_data else None,\n",
    "                datetime.now().isoformat()\n",
    "            ))\n",
    "            \n",
    "            conn.commit()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to log system metric: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_database_stats() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieve comprehensive database statistics\n",
    "    \n",
    "    Why: Provides system monitoring data for database usage, table sizes,\n",
    "    and storage metrics to enable performance monitoring and capacity\n",
    "    planning for Clever's knowledge and interaction storage.\n",
    "    Where: Used by health_monitor.py for system status reporting and\n",
    "    debug_config.py for performance analysis and troubleshooting.\n",
    "    How: Queries table counts across all knowledge base tables using\n",
    "    DatabaseManager and calculates database file size for storage metrics.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Any]: Database statistics including table counts and file size\n",
    "        \n",
    "    Connects to:\n",
    "        - database.py: Uses DatabaseManager for thread-safe statistics queries\n",
    "        - health_monitor.py: Reports database health and usage metrics\n",
    "        - debug_config.py: Provides debugging and performance data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with _db_lock:\n",
    "            with _db_manager._connect() as conn:\n",
    "                cursor = conn.cursor()\n",
    "            \n",
    "            stats = {}\n",
    "            \n",
    "            # Table counts\n",
    "            tables = ['interactions', 'knowledge_sources', 'content_chunks', 'user_preferences', 'personality_state', 'system_metrics']\n",
    "            for table in tables:\n",
    "                try:\n",
    "                    cursor.execute(f'SELECT COUNT(*) FROM {table}')\n",
    "                    count = cursor.fetchone()[0]\n",
    "                    stats[f'{table}_count'] = count\n",
    "                except:\n",
    "                    stats[f'{table}_count'] = 0\n",
    "            \n",
    "            # Database file size using centralized config\n",
    "            import os\n",
    "            if os.path.exists(config.DB_PATH):\n",
    "                stats['database_size_mb'] = os.path.getsize(config.DB_PATH) / (1024 * 1024)\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get database stats: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Initialize database on import - no parameters needed, uses config.DB_PATH\n",
    "init_db()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
